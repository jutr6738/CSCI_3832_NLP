{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Recurrent Neural Networks and Transformers\n",
    "#### CSCI 3832 Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia Troni\n",
    "\n",
    "julia.troni@colorado.edu or jutr6738@colorado.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recurrent Neural Networks\n",
    "\n",
    "In this section, we'll revisit the sentiment analysis problem one final time, this time using an LSTM to classify the movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "\n",
    "import os, random, sys, copy\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, load the Glove embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words from glove\n"
     ]
    }
   ],
   "source": [
    "glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "low = -1.0 / 3\n",
    "high = 1.0 / 3\n",
    "embedding_matrix = np.random.uniform(low=low, high=high, size=(len(embeddings_dict)+1, 50))\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(embeddings_dict.keys(), 1):\n",
    "\n",
    "    word2id[word] = i                                \n",
    "    embedding_matrix[i] = embeddings_dict[word]      \n",
    "\n",
    "word2id['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences from the previous homework, namely that we set the `<pad>` token to be the 0th word in our vocabulary. This is important when working with recurrent networks, as it PyTorch specifically expects that value when calculating losses and packing sequences (unless you specify otherwise). \n",
    "\n",
    "Furthermore, we're no longer initializing our embedding matrix as a matrix of zeros. This doesn't have a practical effect for this homework, since our vocabulary is exactly that of the Glove vocabulary, however it becomes important if we want to add tokens which are not found in Glove (what happens if we embed a word as a vector of zeros?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMovieReviewDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, directory=None, split=None, word2id=None, finalized_data=None, data_limit=250, max_length=256):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        :param directory: The location of aclImdb\n",
    "        :param split: Train or test\n",
    "        :param word2id: The generated glove word2id dictionary\n",
    "        :param finalized_data: We'll use this to initialize a validation set without reloading the data.\n",
    "        :param data_limit: Limiter on the number of examples we load\n",
    "        :param max_length: Maximum length of the sequence\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_limit = data_limit\n",
    "        self.max_length = max_length\n",
    "        self.word2id = word2id\n",
    "\n",
    "        if finalized_data:\n",
    "            self.data = finalized_data\n",
    "\n",
    "        else:\n",
    "\n",
    "            pos_dir = directory + '{}/pos/'.format(split)\n",
    "            neg_dir = directory + '{}/neg/'.format(split)\n",
    "\n",
    "            pos_examples = self.read_folder(pos_dir)\n",
    "            neg_examples = self.read_folder(neg_dir)\n",
    "\n",
    "            pos_examples_tokenized = [(ids, 1) for ids in self.tokenize(pos_examples)]\n",
    "            neg_examples_tokenized = [(ids, 0) for ids in self.tokenize(neg_examples)]\n",
    "\n",
    "            self.data = pos_examples_tokenized + neg_examples_tokenized\n",
    "            random.seed(42)\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def read_folder(self, folder):\n",
    "        examples = []\n",
    "        files = os.listdir(folder)\n",
    "        files.sort()\n",
    "        for fname in files[:self.data_limit]:\n",
    "            with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "                examples.append(f.readline().strip())\n",
    "        return examples\n",
    "\n",
    "    def tokenize(self, examples):\n",
    "\n",
    "        example_ids = []\n",
    "        misses = 0\n",
    "        total = 0\n",
    "        for example in tqdm(examples):\n",
    "            tokens = word_tokenize(example)\n",
    "            ids = []\n",
    "            for tok in tokens:\n",
    "                if tok in word2id:\n",
    "                    ids.append(word2id[tok])\n",
    "                else:\n",
    "                    misses += 1\n",
    "                    ids.append(word2id['unk'])\n",
    "                total += 1\n",
    "\n",
    "            if len(ids) >= self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "                length = self.max_length\n",
    "            else:\n",
    "                length = len(ids)\n",
    "                ids = ids + [word2id['<pad>']]*(self.max_length - len(ids))\n",
    "\n",
    "            example_ids.append((torch.tensor(ids), length))\n",
    "        print('Missed {} out of {} words -- {:.2f}%'.format(misses, total, misses/total))\n",
    "        return example_ids\n",
    "\n",
    "    def generate_validation_split(self, ratio=0.8):\n",
    "\n",
    "        split_idx = int(ratio * len(self.data))\n",
    "\n",
    "        # Take a chunk of the processed data, and return it in order to initialize a validation dataset\n",
    "        validation_split = self.data[split_idx:]\n",
    "\n",
    "        #We'll remove this data from the training data to prevent leakage\n",
    "        self.data = self.data[:split_idx]\n",
    "\n",
    "        return validation_split\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also initialize our movie review dataset again, but with another slight change: while we're still padding the examples to the maximum length we describe, we need to keep track of the original length. This will help us save on some computations down the line. \n",
    "\n",
    "Now we'll load a train and validation dataset. We won't be training any model's in this homework, so we'll only load a couple of examples to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49620a0b5d8d466aae57e90d18302606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed 3285 out of 28372 words -- 0.12%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aa4ff4cd50450dbfa78c904697574e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed 3757 out of 29419 words -- 0.13%\n",
      "Loaded 160 train examples\n",
      "Loaded 40 validation examples\n",
      "((tensor([201535,     10,      8,   5349,     13,     95,     34,    493,    242,\n",
      "          4588,    589,      2,    455,      1,    249,   3499,      3, 201535,\n",
      "         17377,   8210,      1,   1000,      4,   1614,   2833,    754,      6,\n",
      "          4582,  10380,      3, 201535,   6096,  17377, 201535,      8,   2324,\n",
      "           965,      5,   1281,     27,    761,      5,      3, 201535,     64,\n",
      "             2,     54,   1817,    102,    132,      1,  38083,      4,     45,\n",
      "          5297,      2,     35,    416,    215,    386,      1,    621,      3,\n",
      "        201535,    329,      5,    329,      2,     19,  10348,      6,     32,\n",
      "            78,   1715,      4,  17146,      3, 201535,      1,    247,      6,\n",
      "             1,   7659,      2,      1,    301,     15,    922,      6, 201535,\n",
      "         19796,  30411,    275,  12258,  19796,  30411,    275,  12258, 201535,\n",
      "             2,      1,   3172,     33,     59,    572,      2,    859, 201535,\n",
      "        201535,      2,     39,  15722,      1,    492,   8875,      3, 201535,\n",
      "           965,     24, 201535, 201535,     25,     15,   3367,      2,      6,\n",
      "         21970,    563,      2,      6, 121515,      1,  21836,  28463,    144,\n",
      "             3, 201535,     56,  22778,      2,    386,     18,   2535,    276,\n",
      "            26,      1,    922,   2833,   2227,      2, 201535, 201535,     24,\n",
      "            30,  23373,  57706, 201535, 201535,     25,      2,   5310,      8,\n",
      "           192,   4002,   5349,      4,  14122,      2,  13867,      6,    836,\n",
      "             3, 201535,     34,   3043,     38,    320,     10,   3514,  11577,\n",
      "             5, 201535, 201535, 201535, 201535,     24, 201535, 201535, 201535,\n",
      "            25,     90,    554,     38,    320,    856,     71,   1347,   2408,\n",
      "            14,     48,    262,      2,    131,  14481,     15,   3514,     23,\n",
      "        201535,  19796,  30411,    275,  12258,  19796,  30411,    275,  12258,\n",
      "        201535,     92, 229816,     18,      1, 201535, 201535, 201535,      2,\n",
      "            15,      1,   1946,      3, 201535,     16,   2691,      2,  25382,\n",
      "             6,     60,     20,   9328,     20,      8,  48397,      3, 201535,\n",
      "          2053,     16,    412,   6334,     61,      7,      1,    524,      2,\n",
      "           103,     16,      1,  24093]), 256), 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RNNMovieReviewDataset('aclImdb/', 'train', word2id, data_limit=100)\n",
    "validation_examples = train_dataset.generate_validation_split()\n",
    "print('Loaded {} train examples'.format(train_dataset.__len__()))\n",
    "\n",
    "valid_dataset = RNNMovieReviewDataset(finalized_data=validation_examples, word2id=word2id)\n",
    "print('Loaded {} validation examples'.format(valid_dataset.__len__()))\n",
    "\n",
    "print(valid_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the prediction function and training loop as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, valid_dataloader):\n",
    "\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_examples = len(valid_dataloader.dataset)\n",
    "\n",
    "    for (x, x_lengths), y in valid_dataloader:\n",
    "        \n",
    "        output = sigmoid(model(x, x_lengths))\n",
    "        \n",
    "        for i in range(output.shape[0]):\n",
    "            if (output[i] < 0.5 and y[i] == 0) or (output[i] >= 0.5 and y[i] == 1):\n",
    "                total_correct += 1\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_classification(model, train_dataset, valid_dataset, epochs=10, batch_size=32, learning_rate=.001, print_frequency=25):\n",
    "\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    epochs = epochs\n",
    "    batch_size = batch_size\n",
    "    print_frequency = print_frequency\n",
    "\n",
    "    #We'll create an instance of a torch dataloader to collate our data. This class handles batching and shuffling (should be done each epoch)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    print('Total train batches: {}'.format(train_dataset.__len__() / batch_size))\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_model_sd = None\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            (x, x_lengths), y = data\t# Our dataset is returning the input example x and also the lengths of the examples, so we'll unpack that here\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x, x_lengths)\n",
    "\n",
    "            loss = criteria(model_output.squeeze(1), y.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if step % print_frequency == (print_frequency - 1):\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "\n",
    "        print('Evaluating...')\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            acc = predict(model, valid_dataloader)\n",
    "            if acc > best_accuracy: #all diff\n",
    "                best_model_sd = copy.deepcopy(model.state_dict())\n",
    "                best_accuracy = acc\n",
    "\n",
    "    return model.state_dict(), best_model_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences in the training loop compared to the previous homework, namely with how we evaluate during training.\n",
    "\n",
    "**Question (5 points)**: Identify these changes, and describe why might they be useful (consider what might happen if we train for a large number of epochs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "The differences compared to the previous homework: \n",
    "Tracking the best accuracy and best model \n",
    "\n",
    "<code> if acc > best_accuracy: #all diff\n",
    "                best_model_sd = copy.deepcopy(model.state_dict())\n",
    "                best_accuracy = acc\n",
    "                </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define our LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, lstm_hidden_size=50, num_lstm_layers=1, bidirectional=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(input_size = embedding_matrix.shape[1],\n",
    "                            hidden_size = lstm_hidden_size,\n",
    "                            num_layers = num_lstm_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            batch_first = True)\n",
    "        \n",
    "        self.hidden_1 = nn.Linear(lstm_hidden_size * 2, lstm_hidden_size)\n",
    "        self.hidden_2 = nn.Linear(lstm_hidden_size, 1)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_batch, input_lengths):\n",
    "        \n",
    "        print('Input batch shape: {}'.format(input_batch.shape))\n",
    "        embedded_input = self.embedding(input_batch)\n",
    "        \n",
    "        print('Embedded input shape: {}'.format(embedded_input.shape))\n",
    "        packed_input = pack_padded_sequence(embedded_input, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input) # See docs linked below for description of hn.shape\n",
    "        print('hn shape: {}'.format(hn.shape))\n",
    "  \n",
    "        hn_view = hn.view(self.lstm.num_layers, self.num_directions, input_batch.shape[0], self.lstm.hidden_size) # Reshape hn for clarity -- first dimension now represents each layer (total set by num_lstm_layers)\n",
    "        print('hn_view shape: {}'.format(hn_view.shape))\n",
    "        \n",
    "        hn_view_last_layer = hn_view[-1]   # Taking the last layer for our final LSTM output\n",
    "        print('hn_view_last_layer shape: {}'.format(hn_view_last_layer.shape))\n",
    "       \n",
    "        hn_cat = torch.cat([hn_view_last_layer[-2, :, :], hn_view_last_layer[-1, :, :]], dim=1) # Each layer has two directions. We want to use both of these vectors, so concatenate them\n",
    "        print('hn_cat shape: {}'.format(hn_cat.shape))\n",
    "  \n",
    "        hid = self.relu(self.hidden_1(hn_cat))\n",
    "        print('hid shape: {}'.format(hid.shape))\n",
    "  \n",
    "        output = self.hidden_2(hid)\n",
    "        print('output shape: {}'.format(output.shape))\n",
    "  \n",
    "        # raise KeyError\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the forward pass above to print the shapes of the following variables: `input_batch`, `embedded_input`, `hn`, `hn_view`, `hn_view_last_layer`, `hn_cat`, `hid`, `output`. **(5 pts)**\n",
    "\n",
    "Initialize the model and run the model using the block below to print out the shapes (stop the training loop after one forward pass for cleaner outputs). Then, with references to the shapes, describe what each step of the forward pass is doing. **(40 pts)**\n",
    "\n",
    "Hint: For the output shape of the LSTM, remember that we are using a bidrectional LSTM with 2 layers. This means that there is a forward and backward final hidden state for each layer. In our code, `hn` represents the output from the _final timestep_ of the sequence. In general, the first dimension of `hn` when using a multi-layer bidirectional LSTM will be `(layer_1_forward, layer_1_backward, layer_2_forward, layer_2_backward, layer_3_forward, layer_4_backward, ..., layer_n_backward)`.\n",
    "\n",
    "See [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) for the LSTM documentation (check the output section) -- for our code, `batch_first = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train batches: 1.25\n",
      "### Epoch: 1 ###\n",
      "Input batch shape: torch.Size([128, 256])\n",
      "Embedded input shape: torch.Size([128, 256, 50])\n",
      "hn shape: torch.Size([4, 128, 50])\n",
      "hn_view shape: torch.Size([2, 2, 128, 50])\n",
      "hn_view_last_layer shape: torch.Size([2, 128, 50])\n",
      "hn_cat shape: torch.Size([128, 100])\n",
      "hid shape: torch.Size([128, 50])\n",
      "output shape: torch.Size([128, 1])\n",
      "Input batch shape: torch.Size([32, 256])\n",
      "Embedded input shape: torch.Size([32, 256, 50])\n",
      "hn shape: torch.Size([4, 32, 50])\n",
      "hn_view shape: torch.Size([2, 2, 32, 50])\n",
      "hn_view_last_layer shape: torch.Size([2, 32, 50])\n",
      "hn_cat shape: torch.Size([32, 100])\n",
      "hid shape: torch.Size([32, 50])\n",
      "output shape: torch.Size([32, 1])\n",
      "Evaluating...\n",
      "Input batch shape: torch.Size([40, 256])\n",
      "Embedded input shape: torch.Size([40, 256, 50])\n",
      "hn shape: torch.Size([4, 40, 50])\n",
      "hn_view shape: torch.Size([2, 2, 40, 50])\n",
      "hn_view_last_layer shape: torch.Size([2, 40, 50])\n",
      "hn_cat shape: torch.Size([40, 100])\n",
      "hid shape: torch.Size([40, 50])\n",
      "output shape: torch.Size([40, 1])\n",
      "accuracy: 0.475\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(embedding_matrix, lstm_hidden_size=50, num_lstm_layers=2, bidirectional=True)\n",
    "model, best_model = train_lstm_classification(model, train_dataset, valid_dataset, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "\n",
    "1. `input_batch` - shape: [128, 256] - The first dimension is 128 as this represents the batch size. For each element in the batch, we have set the maximum sequence length to be 256\n",
    "2. `embedded_input` - shape: [128, 256, 50] - After passing `input_batch` through the embedding layer, for each example in the batch, every input token is now represented by a vector of size [50]\n",
    "3. `hn` - shape: \n",
    "4. `hn_view` - shape: \n",
    "5. `hn_view_last_layer` = shape: \n",
    "6. `hn_cat` - shape: \n",
    "7. `hid` - shape: \n",
    "8. `output` - shape: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note when training neural networks:\n",
    "- Models can be sensitive to the random seed/starting initialization for the weights. To measure this sensitivity, we can train a number of models with different initializations. To get an overall performance for our model, we'll calculate the accuracy of each instance on the test set, and report the mean. \n",
    "- With enough training, models will almost always begin to overfit to the training set. The most common hallmark of this is if you notice your validation accuracy start to decrease, while your training accuracy continues to improve. To prevent this, we'll keep track of the validation accuracies throughout training -- every time we see a better validation accuracy than previously seen, we'll save the model weights at that time. This way, after training we'll have two sets of model weights: the one with best validation accuracy, and the one after the complete training loop.\n",
    "\n",
    "\n",
    "We've trained 5 LSTM models on the full movie reviews training set (each with a different seed). You can download the model weights from canvas (check for a link in the assignment). The codeblock below shows how to load the saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36452/4073419631.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Set the weights using the saved state dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloaded_model_example\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/model_0.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# The state dictionary is a dictionary matching layers with saved weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[1;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[1;32m-> 1001\u001b[1;33m             \u001b[0mwrap_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m             dtype=dtype)\n\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[0;32m    137\u001b[0m                            \u001b[1;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                            \u001b[1;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# First initialize the model -- right now it'll be random. Don't change these parameters, or the saved weights won't load\n",
    "loaded_model_example = LSTMModel(embedding_matrix, lstm_hidden_size=50, num_lstm_layers=2, bidirectional=True)\n",
    "\n",
    "# Set the weights using the saved state dictionary\n",
    "loaded_model_example.load_state_dict(torch.load('models/model_0.pt'))\n",
    "\n",
    "# The state dictionary is a dictionary matching layers with saved weights\n",
    "example_state_dict = torch.load('models/model_0.pt')\n",
    "print(example_state_dict.keys())\n",
    "\n",
    "# Since we are only using the model for evaluation, we'll set it in evaluation mode so any potential normalization or dropout layers work correctly\n",
    "loaded_model_example.eval()\n",
    "\n",
    "# Now we can evaluate the model on the validation set\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False) \n",
    "predict(loaded_model_example, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 5 trained models (i.e. model_0 .. model_4) has two versions: the fully trained one and the one saved from the best checkpoint (denoted with _best). \n",
    "\n",
    "Complete the following steps **(20 pts)**:\n",
    "\n",
    "1. Load the full test dataset and create a dataloader (example above).\n",
    "2. Load all 10 models, and calculate the accuracy of each model on the test set. \n",
    "3. For each model, report the difference in accuracy between the best checkpoint and fully trained model. Report the mean difference across all models.\n",
    "4. Report the mean and standard deviation of accuracies of the fully trained models.\n",
    "5. Report the mean and standard deviation of accuracies of the best-checkpoint models. \n",
    "6. Create a box-and-whisker plot comparing the the distributions of accuracies across the best-checkpoint and fully trained model. (x-axis will be one of [best-checkpoint|fully-trained], and y-axis will the the box and whisker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Your code here\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Transformers with Huggingface\n",
    "\n",
    "Huggingface is a high-level library which abstracts away most of the implementations we've done in the homeworks -- this includes tokenization, creating a vocabulary, initializing a model, the training loop, saving/loading models, and evaluation. We'll provide a brief introduction to how the library works.\n",
    "\n",
    "For this section, we'll focus on another task: question answering. If you haven't before, install the `transformers`, `datasets`, and `evaluate` libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different flavors of how QA datasets are formulated. Some are _extractive_, where the answers can be found in the context, and are represented by start/end spans. Others are _multiple choice_, where the model is given answer choices and asked to classify them. For today, we'll focus on extractive QA. \n",
    "\n",
    "We'll load [SQuADv1.1](https://rajpurkar.github.io/SQuAD-explorer/), which is a popularly used QA dataset from Stanford. Examples consist of a context, question, and answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd51f286a01a48c68eb1307fb3fd10d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d565b0ee77497faea760bc7e13cd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced7dde619cc4ab9a04ceed993fce7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text to C:/Users/julia/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf369dac0b08402f94385a11aaeeebbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c43f6eea96496ba77a076a30fbec0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a927e2d3606440b9cec9224e6767b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e784f64ee224b73ac25713d31ee4d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9fd094ec7d427a915ca631a762e2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec279f7b3f347bd8ef23bb52ff1b359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to C:/Users/julia/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "Loaded 1000 examples\n",
      "The league announced on October 16, 2012, that the two finalists were Sun Life Stadium and Levi's Stadium. The South Florida/Miami area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010. The San Francisco Bay Area last hosted in 1985 (Super Bowl XIX), held at Stanford Stadium in Stanford, California, won by the home team 49ers. The Miami bid depended on whether the stadium underwent renovations. However, on May 3, 2013, the Florida legislature refused to approve the funding plan to pay for the renovations, dealing a significant blow to Miami's chances.\n",
      "Q: What was the other finalist besides Levi's Stadium?\n",
      "A: Sun Life Stadium\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6bd251f534430999eceadebc795fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e77673fe8841bca9e030488787f902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad', split='validation') # Makes the process of loading datasets much easier than before\n",
    "squad_dataset = squad_dataset.select(random.choices([i for i in range(len(squad_dataset))], k=1000))\n",
    "\n",
    "print('Loaded {} examples'.format(len(squad_dataset)))\n",
    "print(squad_dataset[0]['context'])\n",
    "print('Q: ' + squad_dataset[0]['question'])\n",
    "print('A: ' + squad_dataset[0]['answers']['text'][0])\n",
    "\n",
    "# Load evaluation metric\n",
    "squad_evaluate = load('squad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model and tokenizer.\n",
    "\n",
    "The library offers `pipelines` which handle the conversion of examples to model inputs for us. The `evaluate` library has a wrapped version of the official evaluation script which we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753b94af10374510ab47ff9f57d65e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\julia\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6876d75137ef489caf22e5f53ad2ae26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13edc656813440d0b50aee1eb4ca1586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804cfd692a5d431984d69df6aa29f1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab61777f35d6485ead3b4e360516f25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0791a8ff40794c179aff64309bbbb950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dc560ed63940d7b2997114213b0bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'deepset/roberta-base-squad2'\n",
    "\n",
    "def evaluate_hf_model(model_name):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)       # Initialize the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)                   # Initialize the tokenizer\n",
    "\n",
    "    processor = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def dataset_generator(dataset):\n",
    "        for ex in dataset:\n",
    "            yield (ex,\n",
    "                {'question' : ex['question'], 'context': ex['context']})\n",
    "            \n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Get predictions, and save corresponding reference (if we were using the whole dataset, we wouldn't need this step)\n",
    "    for ex in tqdm(dataset_generator(squad_dataset), total=len(squad_dataset)):\n",
    "\n",
    "        predictions.append({\n",
    "                'id' : ex[0]['id'],\n",
    "                'prediction_text' : processor(ex[1])['answer']\n",
    "        }\n",
    "        )\n",
    "\n",
    "        # In each example, there are multiple possible answers which we compare to. Here we are converting from them from the datasets format to the one expected by the evaluation metric. \n",
    "        references.append({\n",
    "            'id' : ex[0]['id'],\n",
    "            'answers' : [{'text' : z[0], 'answer_start' : z[1]} for z in zip(ex[0]['answers']['text'], ex[0]['answers']['answer_start'])]\n",
    "        })\n",
    "\n",
    "    # Compute metrics\n",
    "    print('Performance of {} : {}'.format(model_name, squad_evaluate.compute(predictions=predictions, references=references)))\n",
    "\n",
    "evaluate_hf_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the [SQuAD dataset page](https://huggingface.co/datasets/squad) on Huggingface. There, you can find models which have been finetuned on the dataset. Load a 3+ models using `model_name` and evaluate them. Plot their performance below (F1 scores), and include a brief description of the model, and what sets it apart from the others you are evaluating (this could be the number of parameters, the training objective, training time, etc.). This information should be available on the model card in Huggingface, or in an associated paper if it exists.  \n",
    "\n",
    "Some ideas: look at distilled/compressed models, models with different training objectives, models that use different training data, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code for plot here (10 pts.)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Describe models here (20 pts.)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e681adf14836894860de42986132a2fbb5bf9e0a673e28b245b6aa439c639a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
